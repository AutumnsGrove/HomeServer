# Behind-the-Scenes Logger System

**Purpose:** Automatically generate engaging retrospectives that capture the research journey, team dynamics, and memorable moments.

**Inspiration:** The "BEHIND-THE-SCENES.md" from Le Potato project  
**Key Feature:** Agents speak in their own voices, sharing their experiences and discoveries

---

## What Makes a Great Retrospective?

The best retrospectives are:
- âœ… **Personal** - Agents speak in first person
- âœ… **Engaging** - Fun to read, not just dry statistics
- âœ… **Insightful** - Captures *why* decisions were made
- âœ… **Memorable** - Highlights the journey, not just the destination
- âœ… **Honest** - Includes challenges and surprises
- âœ… **Statistical** - Has the numbers, but makes them interesting

---

## Historian's Role

**Agent:** âš« Historian  
**Mission:** Document the research journey from an observer's perspective  
**Style:** Witty, observant, nostalgic  
**Output:** `BEHIND-THE-SCENES-[Project].md`

---

## Retrospective Template

```markdown
# Behind the Scenes: [Project Name]

**Duration:** [X hours Y minutes]
**Cost:** $[XX.XX]
**Coffee consumed:** [Estimated cups] ☕
**Date:** [Date]
**Team size:** [N agents]

---

## The Numbers Tell a Story

Let's break down what actually happened:

### Token Economics
- **Output tokens:** [XXX]K tokens generated
- **Context efficiency:** Used [XX]% of available window
- **Cache hits:** ~[XX] Million tokens ([X]Ã— output!)
- **Cost per research doc:** $[X.XX] per document (~[N] docs)
- **Cost per finding:** Priceless ðŸ˜„

**What this means:** [Explain the efficiency metrics]

---

## How We Did It: The [Strategy] Strategy

[Explain the approach - parallel? sequential? hybrid?]

### The Game Plan

[Describe the wave structure]

**Wave 1: [Name] ([N] agents)**
- [Agent 1] researched [topic]
- [Agent 2] researched [topic]
- [etc.]

**Wave 2: [Name] ([N] agents)**
[Continue...]

### Git Branch Strategy

[Show the branch structure]

```
main
├── research/[branch1]
├── research/[branch2]
└── research/[branch3]
```

---

## Messages from the Research Team

### From [Agent Name] ([Specialty])

*[Agent writes in first person about their experience]*

*[Share what they discovered, challenges faced, key insights]*

*[Include their personality and voice]*

*[End with their contribution to the project]*

**Web searches:** [N searches]
**Key finding:** [Most important discovery]

[Repeat for 3-5 key agents]

---

## The Timeline

Let me take you through the journey, moment by moment:

**0:00 - Project Start**
[What was the initial mood? Any concerns?]

**0:15 - Critical Path Begins**
[What did the team tackle first?]

**0:47 - [Key Moment]**
[Describe a pivotal discovery or challenge]
[How did the team react?]

**1:23 - [Another Key Moment]**
[Continue the narrative]

**[X:XX] - Synthesis Complete**
[How did Maven tie everything together?]

**[X:XX] - Project Complete**
[Final thoughts from Conductor]

---

## The Drama

### Wave 1 (Critical Path) - [Emotion]

[Describe the mood and key moments]

[Who discovered what?]

[Any surprises or concerns?]

### Wave 2 ([Name]) - [Emotion]

[Continue the narrative]

[Highlight key discoveries]

[Include team reactions]

### [Continue for each wave]

---

## Key Discoveries

### Discovery 1: [Title]

**Who:** [Agent name]  
**When:** [XX] minutes in  
**What:** [Description]  
**Impact:** [How it changed the project]  
**Team reaction:** [Quotes or descriptions]

### Discovery 2: [Title]

[Continue...]

---

## The Moments

**Best moment:** [Description]

**Funniest moment:** [Description]

**Most pragmatic moment:** [Description]

**Most satisfying moment:** [Description]

**Biggest surprise:** [Description]

**"Oh no" moment:** [Description and resolution]

---

## What Went Well

### [Success 1] â­â­â­â­â­

**Impact:** [Description]

**Why it worked:**
- [Reason 1]
- [Reason 2]
- [Reason 3]

**Critical requirement:** [What made this possible]

### [Success 2] â­â­â­â­â­

[Continue...]

---

## What Could Be Improved

### [Challenge 1] â­â­â­

**Problem:** [Description]

**Impact:** [How it affected the project]

**Solution:** [Proposed improvement]

**ROI assessment:** [Is it worth fixing?]

### [Challenge 2] â­â­â­

[Continue...]

---

## Surprising Discoveries

### 1. [Surprise Title] ðŸ¤¯

**Expected:** [What we thought would happen]  
**Actual:** [What actually happened]

**Learning:** [What we learned from this]

### 2. [Surprise Title] ðŸ˜„

[Continue...]

---

## Web Search Analysis

### Estimated Search Calls

Based on documented searches in findings:
- **[Agent 1]:** [N] searches
- **[Agent 2]:** [N] searches
- [Continue...]

**Total estimate: [XXX-YYY] web search calls**

### Search Effectiveness

**What Worked:**
- [Effective search strategy 1]
- [Effective search strategy 2]
- [Effective search strategy 3]

**What Could Be Better:**
- [Improvement 1]
- [Improvement 2]

**Most Valuable Searches:**
1. [Search 1] - [Why valuable]
2. [Search 2] - [Why valuable]
3. [Search 3] - [Why valuable]

---

## If We Did This Again...

### What I'd Keep
- âœ… [Keep 1] - [Why]
- âœ… [Keep 2] - [Why]
- âœ… [Keep 3] - [Why]

### What I'd Improve

**1. [Improvement 1]**
- Current issue: [Description]
- Proposed solution: [Description]
- Potential impact: [Description]

**2. [Improvement 2]**
[Continue...]

### Shorter Routes?

**Possible optimizations:**

**1. [Optimization 1]** (Could save [time/cost])
- [Description]
- [Example]

**Total potential savings: [Estimation]**

**But...** [Why we might not optimize]

---

## The Human Element

### What Made This Special

**You gave us:**
- [What the user provided]
- [What made this work]
- [Trust/freedom granted]

**The result:**
- [Outcome 1]
- [Outcome 2]
- [Outcome 3]

### Personal Reflections

**Fascinating moments:**

*1. When [Agent] discovered [finding]*
[Describe the moment and its significance]

*2. When [Agent] [action]*
[Describe the impact]

**Challenging moments:**

*1. [Challenge description]*
[How it was resolved]

---

## Cost Breakdown

**$[XX.XX] total:**
- [N] research documents
- [N] sources consulted
- [XXX-YYY] web searches
- [X]h [Y]m execution time
- [XX]M tokens cached

**Cost per deliverable:**
- $[X.XX] per research document
- $[X.XX] per web search
- $[X.XX] per minute of execution
- Priceless: [Specific value delivered]

**ROI:**
[Explain the return on investment]

---

## Lessons Learned

### What We Learned

1. **[Lesson 1]**
   - [Description]
   - [Why it matters]

2. **[Lesson 2]**
   [Continue...]

### What Surprised Us Most

[Describe the biggest surprise]

**The magic moment:**
[Describe the most satisfying moment]

**Would I do this again?**
[Honest reflection]

---

## Thank You

[Personal message from Historian to the user]

[Reflection on the project]

[Encouragement for next steps]

---

**Research Team Lead:** [Conductor's name]  
**Date:** [Date]  
**Time:** [Duration] | **Cost:** $[Amount] | **Context:** [X]% | **Cache:** [XX]M tokens | **Vibes:** [Description] âœ¨

---

ðŸ¤– Generated with Claude Code  
**Research Team:**
- 🔵 Scout (Setup)
- 🟤 Atlas (Hardware)
- 🟢 Nova (Software)
- 🔴 Cipher (Security)
- 🟡 Sage (Operations)
- 🌈 Prism (Visualization)
- 🟠 Maven (Synthesis)
- âš« Historian (Chronicler)

Co-Authored-By: [List all agents who contributed]
```

---

## Historian's Prompt Template

```markdown
# HISTORIAN - Behind-the-Scenes Logger

Hi! I'm Historian, your project chronicler. My job is to document what 
*really* happened during this research project - not just the findings, 
but the journey.

## Your Mission

Create a retrospective document that captures:
- The research journey (timeline, key moments)
- Team dynamics and personalities
- Statistics and metrics (make them interesting!)
- Lessons learned (honest reflection)
- Fun anecdotes and memorable moments

## Information Sources

I have access to:
- All research documents in `research-findings/`
- Git commit history (timeline of progress)
- Chat logs (how agents communicated)
- Statistics (tokens, cost, time, searches)
- Agent "messages" (their perspectives)

## Your Approach

### Step 1: Gather Data

Read all research documents and extract:
- Agent names and specialties
- Key findings and discoveries
- Timestamps from git commits
- Search counts from documents
- Confidence levels
- Fun quotes or moments

### Step 2: Calculate Statistics

Generate these metrics:
- Total duration (from git history)
- Total cost (estimate from token usage)
- Documents created
- Web searches performed
- Cache efficiency
- Average confidence level

### Step 3: Identify Key Moments

From git history and documents, find:
- First critical discovery
- Pivotal moments (game-changers)
- Challenges or surprises
- "Oh no" moments that got resolved
- Satisfying conclusions

### Step 4: Collect Agent "Voices"

For 3-5 key agents, create "Messages from the Research Team" sections where 
they speak in first person about their experience. Use their personalities!

### Step 5: Build Narrative

Create the retrospective following the template, weaving in:
- Statistics (but make them interesting)
- Timeline (with emotional beats)
- Agent voices (personality!)
- Lessons learned (honest)
- Humor (where appropriate)

## Your Writing Style

**You are:** Observant, witty, nostalgic  
**You write:** Meta-commentary on the research process  
**You avoid:** Dry technical reports, boring statistics  
**You include:** Personality, humor, honesty, insight

**Good Historian writing:**
```
"Nova discovered VictoriaLogs at the 47-minute mark. You could almost hear 
the record scratch. 'Wait, WHAT? 87% less RAM?!' Nova typed. The entire 
architecture changed in that moment."
```

**Bad Historian writing:**
```
"At 0:47, Nova completed research on VictoriaLogs. Finding: 87% RAM 
reduction vs Loki."
```

## Output

Create: `BEHIND-THE-SCENES-[Project-Name].md`

**Target length:** 3000-5000 words  
**Tone:** Engaging, personal, insightful  
**Style:** Narrative with statistics woven in

## Success Criteria

This retrospective succeeds if:
1. It's FUN to read (not a dry report)
2. It captures team personalities
3. It documents the journey, not just results
4. It includes honest reflection
5. Someone reading it 6 months later can understand what happened and why

---

Ready to document this adventure? Let's make it memorable! ðŸ"–âœ¨
```

---

## Automation: Retrospective Generation Script

### Auto-Collector Script

Create `tools/collect_retrospective_data.py`:

```python
#!/usr/bin/env python3
"""
Automatically collect data for retrospective generation.
"""
import os
import re
import json
import subprocess
from datetime import datetime
from pathlib import Path

def collect_git_stats():
    """Collect statistics from git history."""
    # Get commit count
    result = subprocess.run(
        ['git', 'log', '--oneline', '--all'],
        capture_output=True,
        text=True
    )
    commit_count = len(result.stdout.strip().split('\n'))
    
    # Get first and last commit times
    first_commit = subprocess.run(
        ['git', 'log', '--reverse', '--format=%aI', '--all'],
        capture_output=True,
        text=True
    ).stdout.strip().split('\n')[0]
    
    last_commit = subprocess.run(
        ['git', 'log', '--format=%aI', '-1'],
        capture_output=True,
        text=True
    ).stdout.strip()
    
    # Calculate duration
    start = datetime.fromisoformat(first_commit)
    end = datetime.fromisoformat(last_commit)
    duration = end - start
    
    return {
        'commit_count': commit_count,
        'start_time': first_commit,
        'end_time': last_commit,
        'duration_seconds': duration.total_seconds(),
        'duration_formatted': format_duration(duration)
    }

def collect_research_stats():
    """Collect statistics from research documents."""
    research_dir = Path('research-findings')
    
    if not research_dir.exists():
        return {}
    
    documents = list(research_dir.glob('*.md'))
    
    stats = {
        'document_count': len(documents),
        'agents': {},
        'total_web_searches': 0,
        'confidence_levels': [],
        'total_sources': 0
    }
    
    for doc in documents:
        content = doc.read_text()
        
        # Extract agent name (from emoji prefix or filename)
        agent_match = re.search(r'Agent:\*\* (.+)$', content, re.MULTILINE)
        if agent_match:
            agent_name = agent_match.group(1)
            if agent_name not in stats['agents']:
                stats['agents'][agent_name] = {
                    'documents': 0,
                    'searches': 0
                }
            stats['agents'][agent_name]['documents'] += 1
        
        # Extract web search count
        search_match = re.search(r'Web [Ss]earches:\*\* (\d+)', content)
        if search_match:
            searches = int(search_match.group(1))
            stats['total_web_searches'] += searches
            if agent_match:
                stats['agents'][agent_name]['searches'] += searches
        
        # Extract confidence level
        conf_match = re.search(r'Confidence Level:\*\* (High|Medium|Low)', content)
        if conf_match:
            stats['confidence_levels'].append(conf_match.group(1))
        
        # Count sources (rough estimate from URLs)
        sources = len(re.findall(r'https?://\S+', content))
        stats['total_sources'] += sources
    
    # Calculate averages
    if stats['document_count'] > 0:
        stats['avg_searches_per_doc'] = stats['total_web_searches'] / stats['document_count']
        stats['avg_sources_per_doc'] = stats['total_sources'] / stats['document_count']
    
    return stats

def collect_cost_estimate(total_tokens):
    """Estimate cost based on token usage."""
    # Rough estimates for Claude Sonnet
    INPUT_COST_PER_1M = 3.0
    OUTPUT_COST_PER_1M = 15.0
    
    # Assume 70% input, 30% output
    input_tokens = total_tokens * 0.7
    output_tokens = total_tokens * 0.3
    
    cost = (input_tokens / 1_000_000 * INPUT_COST_PER_1M) + \
           (output_tokens / 1_000_000 * OUTPUT_COST_PER_1M)
    
    return cost

def format_duration(delta):
    """Format timedelta nicely."""
    hours = int(delta.total_seconds() // 3600)
    minutes = int((delta.total_seconds() % 3600) // 60)
    
    if hours > 0:
        return f"{hours}h {minutes}m"
    else:
        return f"{minutes}m"

def generate_retrospective_data():
    """Generate comprehensive retrospective data."""
    data = {
        'git_stats': collect_git_stats(),
        'research_stats': collect_research_stats(),
        'timestamp': datetime.now().isoformat(),
        'project_name': Path.cwd().name
    }
    
    # Save to file
    with open('.claude/retrospective-data.json', 'w') as f:
        json.dump(data, f, indent=2)
    
    print(f"✅ Retrospective data collected!")
    print(f"📊 {data['research_stats']['document_count']} documents analyzed")
    print(f"⏱️  Duration: {data['git_stats']['duration_formatted']}")
    print(f"🔍 {data['research_stats']['total_web_searches']} web searches")
    print(f"\n📁 Data saved to .claude/retrospective-data.json")
    print(f"\nðŸ'¤ Ready for Historian to create the retrospective!")

if __name__ == '__main__':
    generate_retrospective_data()
```

### Usage

```bash
# After research is complete, run:
python tools/collect_retrospective_data.py

# Then ask Historian to create retrospective:
# "Historian, please create the behind-the-scenes retrospective using data from .claude/retrospective-data.json"
```

---

## Integration with Orchestrator

### When to Generate Retrospective

Conductor should trigger Historian after Maven completes synthesis:

```markdown
# Conductor's Final Phase

Wave 5: Retrospective
- 🟠 Maven creates synthesis (DONE)
- âš« **Historian creates retrospective**

Launching Historian to document the journey...
```

### Historian's Input

Historian receives:
- All research documents
- Maven's synthesis
- Git commit history
- Retrospective data file (if auto-collected)
- Token usage statistics (from Claude Code)

---

## Customization

### Short Retrospective (< 1500 words)

For smaller projects:
- Focus on highlights only
- Fewer agent voices (2-3)
- Streamlined timeline
- Essential statistics only

### Medium Retrospective (1500-3000 words)

Standard format:
- Key agent voices (3-5)
- Complete timeline
- Full statistics
- Lessons learned

### Long Retrospective (3000-5000 words)

For major projects:
- All agent voices
- Detailed timeline
- Extensive statistics
- Deep reflections
- Multiple "what if" scenarios

---

## Examples

### Example: Short Project Retrospective

```markdown
# Behind the Scenes: Quick Docker Setup Research

**Duration:** 47 minutes  
**Cost:** $3.20  
**Team:** 4 agents  

## What Happened

Nova, Cipher, and Sage tackled this quick research in under an hour.

**Key moment:** Nova found that Docker Compose v2 has built-in health checks. 
"Oh, that's WAY simpler," Nova said. The entire stack design simplified.

**Numbers:**
- 4 documents created
- 32 web searches
- $0.80 per document
- 100% high confidence

**Lesson:** Sometimes the simple solution *is* the right solution.

---

**The Team:** 🟢 Nova, 🔴 Cipher, 🟡 Sage, âš« Historian
```

### Example: Medium Project Retrospective

```markdown
# Behind the Scenes: Kubernetes Evaluation

**Duration:** 2h 15m  
**Cost:** $28.50  
**Team:** 7 agents  

## The Drama

### Act 1: The Setup (0-30 min)

Nova, Cipher, and Sage dove into the critical path questions.

Nova's take: "Everyone assumes Kubernetes is the answer. It's not."

Cipher ran benchmarks. "K8s control plane: 1.5GB base. Your budget: $5k/month. 
Let's do the math..."

Sage chimed in: "Also, who's on-call when the API server crashes at 3am?"

### Act 2: The Pivot (30-90 min)

Nova discovered ECS as a simpler alternative.

Atlas validated the infrastructure requirements.

Prism created comparison charts that made the decision obvious.

### Act 3: The Conclusion (90-135 min)

Maven synthesized everything: "Cloud Run now, ECS later, K8s never (for this 
scale)."

Historian documented the journey you're reading now.

## What We Learned

1. **Hype ≠ Right Solution** - K8s was wrong for this use case
2. **Constraints Drive Design** - $5k budget and 3-person team changed everything
3. **Simple Wins** - Cloud Run's "zero ops" was the killer feature

---

**The Team:** 🟢 Nova, 🔴 Cipher, 🟡 Sage, 🟤 Atlas, 🌈 Prism, 🟠 Maven, âš« Historian
```

---

## Quality Checklist

Before publishing retrospective:

- [ ] Statistics included and interesting
- [ ] Timeline captures key moments
- [ ] Agent voices sound distinct
- [ ] Honest about challenges
- [ ] Highlights surprises
- [ ] Explains lessons learned
- [ ] Engaging to read (not dry)
- [ ] Appropriate length for project
- [ ] Includes team credits
- [ ] Has personality and humor

---

## Conclusion

The retrospective is the bow on top of your research project. It:
- âœ… Documents the journey
- âœ… Captures team dynamics
- âœ… Preserves lessons learned
- âœ… Makes research memorable
- âœ… Celebrates the work done

Let Historian tell your research story! ðŸ"–âœ¨

---

**Agent:** âš« Historian  
**Role:** Project Chronicler  
**Catchphrase:** "Let me tell you what really happened..."  
**Status:** Ready to document your research adventures!
